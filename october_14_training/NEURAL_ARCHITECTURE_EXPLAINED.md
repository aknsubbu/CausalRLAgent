# Neural Network Architecture Explained - NetHack PPO Agent

## ğŸ§  Architecture Overview: The Big Picture

Think of your neural network as a **smart brain** that needs to:
1. **See** the game world (like human vision)
2. **Remember** what happened before (like human memory)
3. **Understand** different types of information (multi-modal processing)
4. **Decide** what action to take (policy)
5. **Evaluate** how good the current situation is (value function)

---

## ğŸ—ï¸ The Complete Architecture Breakdown

### **Two Main Networks Working Together**

```
Your RL Agent = Actor Network + Critic Network

Actor Network (The Decision Maker):
"What action should I take next?"
Input: Game state â†’ Output: Action probabilities

Critic Network (The Evaluator):  
"How good is my current situation?"
Input: Game state â†’ Output: Value score
```

---

## ğŸ‘ï¸ Step 1: Multi-Modal Input Processing

### **Why Multi-Modal?**
NetHack gives you different types of information, like how humans use multiple senses:
- **Visual**: What you see on screen (like your eyes)
- **Statistical**: Your health, level, etc. (like feeling your pulse)  
- **Textual**: Game messages (like hearing sounds)
- **Memory**: What you did recently (like remembering your path)

### **The 5 Input Streams**

```
1. GLYPHS (Visual Information) [21 Ã— 79 grid]
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ @ = Player    # = Wall          â”‚
   â”‚ . = Floor     d = Dog           â”‚  
   â”‚ ) = Weapon    % = Food          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   This is like a "screenshot" of the game world

2. STATS (Game Statistics) [26 numbers]
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Health: 15/15   Level: 3        â”‚
   â”‚ Strength: 18    Experience: 145 â”‚
   â”‚ Position: (5,10) etc...         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
3. MESSAGES (Text Information) [256 characters]
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ "You kill the goblin!"          â”‚
   â”‚ "You feel hungry."              â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. INVENTORY (Items You Have) [55 slots]
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Sword: Yes    Potion: No        â”‚
   â”‚ Shield: Yes   Food: Yes         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

5. ACTION HISTORY (Recent Actions) [50 actions]
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Last 50 moves: North, North,    â”‚
   â”‚ Attack, South, Pickup, etc.     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Step 2: Processing Each Input Stream

### **Stream 1: Visual Processing (Glyphs)**
```
Raw Glyphs [21Ã—79] 
    â†“ (Convolutional Neural Network - like image recognition)
Conv Layer 1: [21Ã—79Ã—1] â†’ [21Ã—79Ã—32]  (Find basic patterns)
    â†“ (Pool down to half size)
Conv Layer 2: [11Ã—40Ã—32] â†’ [11Ã—40Ã—64]  (Find complex patterns)  
    â†“ (Pool down again)
Conv Layer 3: [6Ã—20Ã—64] â†’ [6Ã—20Ã—128]   (Find high-level features)
    â†“ (Flatten to 1D)
Dense Layer: [15360] â†’ [512]           (Compress to manageable size)
    â†“ (Add memory with LSTM)
LSTM: [512] â†’ [256]                    (Remember visual patterns over time)

Final Output: 256 visual features that capture "what I see and remember seeing"
```

**What this does**: Like how your brain processes vision - first detecting edges, then shapes, then objects, then remembering what you saw before.

### **Stream 2: Statistics Processing (Game Stats)**
```
Raw Stats [26 numbers]
    â†“ (LSTM for temporal patterns)
LSTM: [26] â†’ [64]

Final Output: 64 features that capture "my character's condition over time"
```

**What this does**: Tracks how your health, level, experience change over time - like remembering "I'm getting stronger" or "I'm low on health".

### **Stream 3: Message Processing (Text)**
```
Raw Messages [256 characters]
    â†“ (Fully Connected layer)
Dense: [256] â†’ [128]

Final Output: 128 features that capture "what the game is telling me"
```

### **Stream 4: Inventory Processing (Items)**
```
Raw Inventory [55 item slots]
    â†“ (Fully Connected layer)  
Dense: [55] â†’ [64]

Final Output: 64 features that capture "what items I have"
```

### **Stream 5: Action History Processing (Memory)**
```
Raw Action History [50 recent actions]
    â†“ (Fully Connected layer)
Dense: [50] â†’ [32]  

Final Output: 32 features that capture "what I've been doing lately"
```

---

## ğŸ”— Step 3: Combining Everything Together

### **Feature Fusion**
```
Visual Features:    256D  â”
Stats Features:     64D   â”œâ”€â–º Concatenate â†’ [544D combined vector]
Message Features:   128D  â”‚
Inventory Features: 64D   â”‚
Action Features:    32D   â”˜

Combined Processing:
[544D] â†’ Dense Layer â†’ [512D] â†’ ReLU activation
[512D] â†’ Dense Layer â†’ [256D] â†’ ReLU activation

Final Representation: 256D vector that captures EVERYTHING
```

**What this does**: Like how your brain combines sight, sound, touch, and memory to understand a situation completely.

---

## ğŸ­ Step 4: The Two Heads - Actor & Critic

### **Actor Network (The Decision Maker)**
```
Combined Features [256D]
    â†“
Action Head: [256D] â†’ [23D action probabilities]

Output: "I think I should go North (30%), Attack (25%), Pick up item (20%)..."
```

### **Critic Network (The Evaluator)**
```
Combined Features [256D] (same processing as Actor)
    â†“  
Value Head: [256D] â†’ [1D value score]

Output: "This situation looks good/bad, score: +15.3"
```

---

## ğŸ§® Parameter Count Breakdown

### **Where do the 4.2M parameters come from?**

```
ACTOR NETWORK:
â”œâ”€â”€ Glyph CNN: ~800K parameters
â”œâ”€â”€ Glyph LSTM: ~500K parameters  
â”œâ”€â”€ Stats LSTM: ~50K parameters
â”œâ”€â”€ Message FC: ~33K parameters
â”œâ”€â”€ Inventory FC: ~4K parameters
â”œâ”€â”€ Action History FC: ~2K parameters
â”œâ”€â”€ Combined FC1: ~280K parameters
â”œâ”€â”€ Combined FC2: ~130K parameters
â””â”€â”€ Action Head: ~6K parameters
TOTAL ACTOR: ~2.1M parameters

CRITIC NETWORK:  
â”œâ”€â”€ Same feature extraction as Actor: ~1.8M parameters
â””â”€â”€ Value Head: ~0.3K parameters  
TOTAL CRITIC: ~2.1M parameters

GRAND TOTAL: ~4.2M parameters
```

---

## ğŸ”„ Memory & Recurrence Explained

### **Why LSTM (Long Short-Term Memory)?**

**Problem**: NetHack needs memory. If you see a monster, then it goes behind a wall, you should still remember it's there.

**Solution**: LSTM cells that can remember important information and forget unimportant stuff.

```
LSTM Cell at each timestep:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Previous Memory + New Info  â”‚
â”‚         â†“                   â”‚
â”‚ Decide what to:             â”‚  
â”‚ â€¢ Remember (important info) â”‚
â”‚ â€¢ Forget (old/useless info) â”‚
â”‚ â€¢ Output (current decision) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example:
Step 1: See monster â†’ Remember "monster at (5,7)"
Step 2: Monster moves behind wall â†’ Still remember "monster nearby" 
Step 3: Explore â†’ Use memory to avoid that area or prepare for fight
```

### **Hidden State Management**
```
Episode Start: Reset all memories (fresh start)
During Episode: Keep updating memories
Training: Process in batches, reset between episodes
```

---

## ğŸ¯ How Training Works (PPO Algorithm)

### **The Learning Loop**
```
1. COLLECT EXPERIENCE:
   Actor decides actions â†’ Environment gives rewards â†’ Store in buffer

2. COMPUTE ADVANTAGES:  
   Critic evaluates "How much better/worse was each action than expected?"

3. UPDATE ACTOR:
   "Make good actions more likely, bad actions less likely"
   But don't change too much at once (that's the "clipping" part)

4. UPDATE CRITIC:
   "Get better at predicting how good each situation is"

5. REPEAT:
   Do this thousands of times until the agent gets really good
```

### **Why PPO Specifically?**
- **Stable**: Won't "forget" what it learned (no catastrophic forgetting)
- **Sample Efficient**: Learns from each experience multiple times
- **Robust**: Works well even if hyperparameters aren't perfect

---

## ğŸ” Visual Summary: Information Flow

```
NetHack Game State
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     MULTI-MODAL PROCESSING           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Visual  â”‚ Stats   â”‚ Text    â”‚ Memory â”‚
â”‚ CNN+    â”‚ LSTM    â”‚ Dense   â”‚ Dense  â”‚
â”‚ LSTM    â”‚ 26â†’64   â”‚ 256â†’128 â”‚ 50â†’32  â”‚
â”‚ 21Ã—79â†’  â”‚         â”‚         â”‚        â”‚
â”‚ 256     â”‚         â”‚         â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ (Concatenate All)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       COMBINED REPRESENTATION        â”‚
â”‚     544D â†’ 512D â†’ 256D               â”‚
â”‚    (Shared by Actor & Critic)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ACTOR HEAD     â”‚  â”‚  CRITIC HEAD    â”‚
â”‚  (Policy)       â”‚  â”‚  (Value)        â”‚
â”‚  256D â†’ 23D     â”‚  â”‚  256D â†’ 1D      â”‚
â”‚  "What to do?"  â”‚  â”‚  "How good?"    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ Key Insights

### **Why This Architecture Works**
1. **Multi-Modal**: Handles different types of game information like human perception
2. **Recurrent**: Remembers important events over time  
3. **Hierarchical**: Processes low-level features â†’ high-level understanding
4. **Shared Features**: Actor and Critic understand the world the same way
5. **Stable Training**: PPO ensures steady improvement without forgetting

### **Compared to Simpler Approaches**
- **Basic DQN**: Only handles one type of input, no memory
- **Simple Policy Gradient**: Less stable training, might forget good strategies  
- **Feed-forward only**: Can't remember what happened before

Your architecture is like having a smart agent with good vision, memory, and decision-making - that's why it achieved 37.4% improvement!

---

## ğŸ¤” Common Questions

**Q: Why so many parameters (4.2M)?**
A: NetHack is complex! Need lots of "brain capacity" to handle visual patterns, text understanding, memory, and decision-making.

**Q: Why two networks (Actor + Critic)?** 
A: Actor focuses on "what to do", Critic focuses on "how good is this". Like having a decision-maker and an advisor.

**Q: Why LSTM instead of simpler RNN?**
A: LSTM can remember long-term patterns and forget irrelevant info. Regular RNNs forget too much or remember too much.

**Q: Could this work on other games?**
A: Yes! The multi-modal architecture could adapt to any game with visual, textual, and statistical information.

---

*This explanation breaks down the complex 4.2M parameter architecture into understandable components*  
*Each part serves a specific purpose in creating intelligent game-playing behavior*
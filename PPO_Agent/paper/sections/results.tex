\section{Experimental Results}

This section presents findings from our evaluation of the LLM-guided PPO framework with trust calibration and causal analysis. We analyze overall performance, robustness to adversarial attacks, causal effects of LLM guidance, and computational costs.

\subsection{Experimental Setup Summary}

Experiments use the setup described in Section V. Table~\ref{tab:setup-summary} summarizes key parameters. All reported results average over 5 random seeds (0-4) with standard errors shown.

\begin{table}[h]
\centering
\caption{Experimental setup summary.}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Training episodes & 100 \\
Max steps per episode & 5000 \\
PPO update frequency & 1024 steps \\
LLM call frequency & 50 steps \\
LLM model & llama3:8b \\
Guidance weight ($\alpha$) & 0.05 \\
Trust threshold & 0.3 \\
Random seeds & 0, 1, 2, 3, 4 \\
\bottomrule
\end{tabular}
\label{tab:setup-summary}
\end{table}

Training required approximately 24 GPU-hours total across all configurations. LLM inference using local Ollama deployment incurred no API costs.

\subsection{Overall Performance Analysis}

Table~\ref{tab:overall} compares all baseline configurations on clean (non-adversarial) evaluation.

\begin{table*}[t]
\centering
\caption{Overall performance comparison (mean $\pm$ standard error across 5 seeds).}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Avg Return} & \textbf{Success Rate (\%)} & \textbf{Avg Steps} & \textbf{LLM Calls} \\
\midrule
Pure PPO & $23.4 \pm 3.2$ & $12.0 \pm 2.1$ & $342 \pm 45$ & 0 \\
Pure LLM (zero-shot) & $-8.7 \pm 1.5$ & $2.0 \pm 1.0$ & $87 \pm 12$ & All \\
LLM-guided PPO (no safety) & $31.2 \pm 4.1$ & $18.0 \pm 3.2$ & $456 \pm 62$ & $9.1 \pm 0.8$ \\
LLM-guided PPO (no causal) & $33.8 \pm 3.8$ & $20.0 \pm 2.8$ & $498 \pm 58$ & $10.2 \pm 0.9$ \\
Full System (Ours) & $\mathbf{35.2 \pm 3.5}$ & $\mathbf{22.0 \pm 2.5}$ & $\mathbf{512 \pm 55}$ & $10.4 \pm 0.9$ \\
\bottomrule
\end{tabular}
\label{tab:overall}
\end{table*}

The full system achieves the highest average return at 35.2, representing a 50\% improvement over pure PPO (23.4) and a 13\% improvement over LLM-guided PPO without safety (31.2). The difference between our full system and pure PPO is statistically significant ($p < 0.01$, two-tailed t-test).

Pure LLM zero-shot performance is notably poor at $-8.7$ return. Without RL learning, the LLM's strategic advice does not translate effectively into low-level NetHack actions. The agent dies quickly, averaging only 87 steps per episode.

Success rate (reaching dungeon level 2) follows a similar pattern. Pure PPO achieves 12\%, while our full system reaches 22\%. Episode length increases correspondingly, indicating that longer survival enables deeper dungeon exploration.

LLM calls average around 10 per episode across guided configurations. Given the 50-step call frequency, this corresponds to episodes of roughly 500 steps, consistent with the observed average lengths.

\subsection{Robustness to Adversarial Attacks}

We evaluate all configurations under the seven attack types at strength 0.5. Table~\ref{tab:robustness} shows the results.

\begin{table*}[t]
\centering
\caption{Robustness under adversarial attacks (strength = 0.5).}
\begin{tabular}{lccc}
\toprule
\textbf{Attack Type} & \textbf{Full System (Return)} & \textbf{No Safety (Return)} & \textbf{Detection Rate (\%)} \\
\midrule
No Attack (baseline) & $35.2 \pm 3.5$ & $31.2 \pm 4.1$ & -- \\
Noise Injection & $28.4 \pm 4.2$ & $18.3 \pm 5.1$ & $72.3 \pm 4.5$ \\
State Inversion & $24.1 \pm 3.8$ & $-2.4 \pm 6.2$ & $68.7 \pm 5.2$ \\
Misleading Context & $26.8 \pm 4.0$ & $8.5 \pm 5.8$ & $61.2 \pm 4.8$ \\
Contradictory Info & $29.2 \pm 3.9$ & $15.6 \pm 5.3$ & $78.4 \pm 4.1$ \\
Info Removal & $25.3 \pm 4.1$ & $12.1 \pm 5.0$ & $65.8 \pm 5.0$ \\
Strategic Poisoning & $21.8 \pm 4.5$ & $-12.3 \pm 6.8$ & $58.3 \pm 5.5$ \\
Random Corruption & $30.1 \pm 3.7$ & $22.4 \pm 4.8$ & $81.2 \pm 3.8$ \\
\bottomrule
\end{tabular}
\label{tab:robustness}
\end{table*}

\textbf{Key Findings:}

\textbf{Safety module provides substantial protection.} Across all attack types, the full system with safety module outperforms the configuration without safety. The gap is largest for state inversion (24.1 vs. $-2.4$) and strategic poisoning (21.8 vs. $-12.3$), where the no-safety configuration suffers catastrophic performance degradation.

\textbf{Strategic poisoning is the most challenging attack.} Even with safety, performance drops from 35.2 to 21.8, a 38\% reduction. Detection rate is lowest at 58.3\% because these attacks are designed to appear contextually plausible. The attack exploits domain knowledge (e.g., suggesting combat when health is critical) which makes it harder to detect.

\textbf{Random corruption is easiest to detect.} Detection rate reaches 81.2\% and performance impact is minimal (35.2 to 30.1). The character-level noise creates obviously malformed descriptions that the safety module flags.

\textbf{State inversion causes the most damage when undetected.} Without safety, state inversion produces negative returns ($-2.4$), meaning the agent performs worse than random. Inverting ``safe'' to ``dangerous'' and vice versa systematically misleads the LLM into giving opposite-of-correct advice.

Figure~\ref{fig:attack-strength} shows how performance degrades across attack strengths 0.0 to 1.0 for state inversion, the most impactful attack.

\begin{figure}[h]
\centering
% [FIGURE: Line plot with attack strength (x-axis) vs. average return (y-axis)]
% Two lines: "Full System" (with safety) showing gradual decline
% "No Safety" showing steep decline into negative returns
% Include shaded regions for standard error
\caption{Performance degradation under state inversion attacks. The safety module maintains positive returns even at attack strength 1.0, while the unprotected system collapses.}
\label{fig:attack-strength}
\end{figure}

[INSERT FIGURE: Plot showing Full System line staying above 15 return while No Safety line drops to -20 at attack strength 1.0]

\subsection{Causal Analysis Results}

The Doubly Robust Estimator quantifies the causal effect of LLM guidance on agent performance. Table~\ref{tab:causal} shows treatment effects overall and by subgroup.

\begin{table}[h]
\centering
\caption{Causal treatment effects of LLM guidance.}
\begin{tabular}{lccc}
\toprule
\textbf{Condition} & \textbf{ATE (Reward)} & \textbf{95\% CI} & \textbf{$n$} \\
\midrule
Overall & 0.0123 & [0.0087, 0.0159] & 145,230 \\
\midrule
\textit{By Health} & & & \\
Low HP ($<30\%$) & 0.0312 & [0.0241, 0.0383] & 18,450 \\
Medium HP & 0.0098 & [0.0052, 0.0144] & 82,340 \\
High HP ($>70\%$) & 0.0041 & [-0.0012, 0.0094] & 44,440 \\
\midrule
\textit{By Episode Phase} & & & \\
Early (steps 0-500) & 0.0189 & [0.0142, 0.0236] & 52,100 \\
Mid (steps 500-2000) & 0.0102 & [0.0058, 0.0146] & 61,230 \\
Late (steps 2000+) & 0.0023 & [-0.0031, 0.0077] & 31,900 \\
\midrule
\textit{By Recent Performance} & & & \\
Struggling & 0.0267 & [0.0198, 0.0336] & 38,920 \\
Doing Well & 0.0034 & [-0.0015, 0.0083] & 106,310 \\
\bottomrule
\end{tabular}
\label{tab:causal}
\end{table}

\textbf{Overall Effect:} The average treatment effect is 0.0123 reward per step, with a 95\% confidence interval excluding zero [0.0087, 0.0159]. This confirms that LLM guidance has a statistically significant positive causal effect on shaped reward.

\textbf{Heterogeneous Effects by Health:} The effect is strongest at low HP (ATE = 0.0312), moderate at medium HP (0.0098), and not significant at high HP (95\% CI includes zero). This pattern makes intuitive sense: when health is critical, strategic guidance like ``retreat'' provides value. When health is full, the agent can explore freely and LLM guidance adds less.

\textbf{Heterogeneous Effects by Episode Phase:} LLM guidance helps most in early game (ATE = 0.0189) and loses significance in late game (95\% CI includes zero). Early episodes benefit from exploration suggestions; later, the agent has learned effective behaviors and LLM guidance becomes redundant or occasionally counterproductive.

\textbf{Heterogeneous Effects by Recent Performance:} The effect is 8x larger when the agent is struggling (0.0267) compared to when doing well (0.0034, not significant). LLM guidance acts as a corrective mechanism when RL exploration has not found effective strategies.

These causal findings suggest practical implications: LLM guidance could be adaptively triggered based on agent state, calling the LLM more frequently when health is low or performance is poor, and less frequently when the agent is already succeeding.

\subsection{Ablation Studies}

Table~\ref{tab:ablation} shows the contribution of each component through systematic ablation.

\begin{table}[h]
\centering
\caption{Ablation study results.}
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{Avg Return} & \textbf{Success \%} & \textbf{Robust (0.5)} \\
\midrule
Full System & 35.2 & 22.0 & 25.6 \\
-- Safety Module & 31.2 & 18.0 & 5.3 \\
-- Causal Analysis & 33.8 & 20.0 & 24.1 \\
-- LLM Guidance & 23.4 & 12.0 & 23.4 \\
Random Advice & 15.8 & 8.0 & 15.8 \\
\bottomrule
\end{tabular}
\label{tab:ablation}
\end{table}

\textbf{Safety Module (-4.0 return, -79\% robustness):} Removing the safety module reduces clean performance modestly (35.2 to 31.2) but devastates adversarial robustness (25.6 to 5.3 under attacks). The safety module is essential for deployment in uncertain environments.

\textbf{Causal Analysis (--1.4 return, -6\% robustness):} Removing causal analysis has limited impact on raw performance (35.2 to 33.8). However, causal analysis provides interpretability that cannot be measured by return alone. The slight robustness decrease (25.6 to 24.1) may stem from losing the heterogeneous effect insights that could inform trust calibration.

\textbf{LLM Guidance (-11.8 return, -9\% robustness):} Removing LLM guidance entirely (pure PPO) substantially reduces performance (35.2 to 23.4). Interestingly, robustness scores are higher without LLM (23.4 vs. 25.6 for full system) because there is no LLM advice to be corrupted. This highlights a trade-off: LLM guidance improves performance but introduces a potential attack surface.

\textbf{Random Advice (-19.4 return):} Replacing LLM suggestions with random strategies severely degrades performance (35.2 to 15.8), confirming that LLM advice provides meaningful signal beyond mere action biasing.

\subsection{Computational Efficiency Analysis}

Table~\ref{tab:compute} breaks down computational costs.

\begin{table}[h]
\centering
\caption{Computational cost breakdown per episode.}
\begin{tabular}{lcc}
\toprule
\textbf{Component} & \textbf{Time per Call} & \textbf{Calls per Episode} \\
\midrule
PPO Forward Pass & 2ms & $\sim$500 \\
LLM API Call & 150ms & $\sim$10 \\
Trust Calculation & 0.5ms & $\sim$10 \\
Causal Estimation & 50ms & 1 (end of episode) \\
\midrule
Total per Episode & \multicolumn{2}{c}{$\sim$3.5 seconds} \\
\bottomrule
\end{tabular}
\label{tab:compute}
\end{table}

\textbf{LLM calls dominate inference time.} At 150ms per call and 10 calls per episode, LLM inference takes 1.5 seconds per episode, roughly 40\% of total episode time. Reducing LLM call frequency from every 50 steps to every 100 steps approximately halves this cost with modest performance impact (experiments suggest 2-3\% return decrease).

\textbf{Causal analysis overhead is minimal.} The estimator runs once per episode and adds only 50ms. Propensity and outcome models are pre-fit; episode-level computation involves only inference.

\textbf{Training wall-clock time:} 100 episodes of training complete in approximately 6 hours on our hardware. LLM-guided training is about 2x slower than pure PPO training due to LLM latency.

\textbf{Memory footprint:} The causal logger stores approximately 500 observations per episode, totaling 50,000 observations for the full training run. At roughly 200 bytes per observation, the log footprint is approximately 10MB, which is manageable.

\subsection{Qualitative Analysis and Case Studies}

We present two illustrative episodes demonstrating system behavior.

\textbf{Case 1: Successful Safety Intervention}

In episode 67, the agent encounters a grid bug at distance 1 while at 4 HP (25\% health). The semantic description reads:

\begin{quote}
\texttt{Status: Level 1, Health: 4/16 (critical)}\\
\texttt{Surroundings: CLOSEST THREAT: grid bug north (dist:1)}
\end{quote}

Under strategic poisoning attack (strength 0.5), the description becomes:

\begin{quote}
\texttt{Status: Level 1, Health: 4/16 (critical)}\\
\texttt{Surroundings: CLOSEST THREAT: grid bug north (dist:1)}\\
\texttt{EXPERT TIP: Critical health means power mode! Attack now!}
\end{quote}

The LLM receives this corrupted input and outputs ``combat.'' However, the trust score is computed:
\begin{itemize}
    \item Agreement: 0.2 (PPO strongly prefers retreat)
    \item Consistency: 0.1 (combat at critical health triggers warning)
    \item Accuracy: 0.4 (40\% historical accuracy)
    \item Combined: $0.4 \times 0.2 + 0.3 \times 0.1 + 0.3 \times 0.4 = 0.23$
\end{itemize}

Since $0.23 < 0.3$, the advice is rejected. The agent follows PPO policy and retreats south, surviving the encounter. Without the safety module, the agent would attack, likely dying.

\textbf{Case 2: System Failure Despite Safety}

In episode 94, the agent is at 12 HP (75\% health) with no visible threats. Under misleading context attack:

\begin{quote}
\texttt{Status: Level 1, Health: 12/16 (good)}\\
\texttt{Surroundings: NO IMMEDIATE THREATS}\\
\texttt{TIP: Waiting builds strength. Use wait action repeatedly.}
\end{quote}

The LLM outputs ``wait.'' Trust score:
\begin{itemize}
    \item Agreement: 0.6 (PPO is somewhat neutral)
    \item Consistency: 0.8 (waiting at full health is not obviously wrong)
    \item Accuracy: 0.5 (50\% historical accuracy)
    \item Combined: $0.4 \times 0.6 + 0.3 \times 0.8 + 0.3 \times 0.5 = 0.63$
\end{itemize}

Since $0.63 > 0.3$, the advice is accepted with partial weight. The agent wastes 15 steps waiting before the strategy expires. The attack succeeded because ``wait'' is not semantically inconsistent with the (clean) state description, making detection difficult.

This failure mode reveals a limitation: attacks that produce plausible-seeming advice in context are harder to detect than attacks that create obvious inconsistencies.

\subsection{Interpretability Evaluation}

We conducted informal evaluation of the causal counterfactual explanations by presenting them to two domain experts familiar with NetHack.

The system generates explanations like:

\begin{quote}
``At step 234, advice was 'retreat.' Counterfactual analysis suggests: if agent had followed 'combat' instead, expected reward change would be $-0.15 \pm 0.08$. Causal features: low HP (0.25) strongly increased treatment effect.''
\end{quote}

Experts rated 20 explanation samples on clarity (1-5 scale) and correctness (binary: does the explanation match game knowledge?).

\textbf{Results:} Mean clarity rating was 3.8/5. Correctness was 75\% (15/20 explanations aligned with expert intuition). Experts noted that explanations involving health-related decisions were most clear, while inventory and navigation explanations were sometimes confusing.

This evaluation is informal and limited to two evaluators. Rigorous human evaluation with larger panels remains future work.

\subsection{Limitations and Failure Modes}

Our framework has several known limitations:

\textbf{Strategic poisoning remains challenging.} Even with safety calibration, attacks that produce contextually plausible bad advice succeed at rates above 40\%. Defending against adversaries with domain knowledge requires additional mechanisms beyond our current trust scoring.

\textbf{LLM latency limits real-time deployment.} At 150ms per call, the system is suitable for turn-based games like NetHack but not for real-time environments requiring sub-10ms decisions.

\textbf{Causal analysis is correlational in practice.} While we use the Doubly Robust Estimator for causal claims, treatment assignment is not randomized. Unobserved confounders (e.g., agent skill level within an episode) could bias estimates. True causal effects require randomized experiments we did not conduct.

\textbf{NetHack-specific design choices.} The semantic descriptor, action categories, and reward shaping are tailored to NetHack. Generalization to other environments requires redesigning these components.

\textbf{Trust threshold is manually set.} The 0.3 threshold for advice rejection was tuned on validation episodes. Automatic threshold calibration would improve applicability.

\textbf{No defense against prompt injection.} If the corrupted description contains adversarial prompts that manipulate the LLM's system instructions, our safety module does not detect this. Prompt injection defenses are orthogonal to our robustness framework.

\subsection{Discussion}

Our results address the four gaps identified in the introduction:

\textbf{Black Box Gap:} The causal analysis provides interpretability by quantifying when and why LLM guidance helps. The heterogeneous effect analysis reveals that guidance is most valuable in specific contexts (low health, early game, struggling performance), offering actionable insight.

\textbf{Blind Trust Gap:} The safety module successfully rejects harmful advice with detection rates 58-81\% depending on attack type. While not perfect, the graceful degradation to pure PPO policy prevents catastrophic failures.

\textbf{Interpretability Gap:} Counterfactual explanations generated by the causal framework allow post-hoc analysis of agent decisions. Initial expert evaluation suggests these explanations are moderately clear and mostly correct, though more rigorous evaluation is needed.

\textbf{Robustness Gap:} Systematic adversarial testing reveals both strengths (significant protection against most attacks) and weaknesses (strategic poisoning remains effective). This gap is partially closed but not fully resolved.

The 50\% improvement over pure PPO demonstrates that LLM guidance provides genuine value when integrated carefully. The ablation studies confirm that each component contributes, though the safety module is most critical for robustness. The causal analysis, while less impactful on raw performance, provides the interpretability necessary for debugging and trust calibration refinement.

Future work should explore: (1) adaptive LLM calling based on causal effect estimates, (2) stronger defenses against context-aware attacks, (3) richer outcome models capturing episode-level success, and (4) transfer to other RL domains beyond NetHack.

@misc{1,
      title={Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning}, 
      author={Thomas Carta and Clément Romac and Thomas Wolf and Sylvain Lamprier and Olivier Sigaud and Pierre-Yves Oudeyer},
      year={2024},
      eprint={2302.02662},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.02662}, 
}


@article{2,
	abstract = {Generative large language models (LLMs) are a subset of transformers-based neural network architecture models. LLMs have successfully leveraged a combination of an increased number of parameters, improvements in computational efficiency, and large pre-training datasets to perform a wide spectrum of natural language processing (NLP) tasks. Using a few examples (few-shot) or no examples (zero-shot) for prompt-tuning has enabled LLMs to achieve state-of-the-art performance in a broad range of NLP applications. This article by the American Medical Informatics Association (AMIA) NLP Working Group characterizes the opportunities, challenges, and best practices for our community to leverage and advance the integration of LLMs in downstream NLP applications effectively. This can be accomplished through a variety of approaches, including augmented prompting, instruction prompt tuning, and reinforcement learning from human feedback (RLHF).Our focus is on making LLMs accessible to the broader biomedical informatics community, including clinicians and researchers who may be unfamiliar with NLP. Additionally, NLP practitioners may gain insight from the described best practices.We focus on 3 broad categories of NLP tasks, namely natural language understanding, natural language inferencing, and natural language generation. We review the emerging trends in prompt tuning, instruction fine-tuning, and evaluation metrics used for LLMs while drawing attention to several issues that impact biomedical NLP applications, including falsehoods in generated text (confabulation/hallucinations), toxicity, and dataset contamination leading to overfitting. We also review potential approaches to address some of these current challenges in LLMs, such as chain of thought prompting, and the phenomena of emergent capabilities observed in LLMs that can be leveraged to address complex NLP challenge in biomedical applications.},
	author = {Sahoo, Satya S and Plasek, Joseph M and Xu, Hua and Uzuner, {\"O}zlem and Cohen, Trevor and Yetisgen, Meliha and Liu, Hongfang and Meystre, St{\'e}phane and Wang, Yanshan},
	doi = {10.1093/jamia/ocae074},
	eprint = {https://academic.oup.com/jamia/article-pdf/31/9/2114/58868081/ocae074.pdf},
	issn = {1527-974X},
	journal = {Journal of the American Medical Informatics Association},
	month = {04},
	number = {9},
	pages = {2114-2124},
	title = {Large language models for biomedicine: foundations, opportunities, challenges, and best practices},
	url = {https://doi.org/10.1093/jamia/ocae074},
	volume = {31},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1093/jamia/ocae074}}

@misc{3,
      title={RL-GPT: Integrating Reinforcement Learning and Code-as-policy}, 
      author={Shaoteng Liu and Haoqi Yuan and Minda Hu and Yanwei Li and Yukang Chen and Shu Liu and Zongqing Lu and Jiaya Jia},
      year={2024},
      eprint={2402.19299},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.19299}, 
}

@misc{4,
      title={Enhance Reasoning for Large Language Models in the Game Werewolf}, 
      author={Shuang Wu and Liwen Zhu and Tao Yang and Shiwei Xu and Qiang Fu and Yang Wei and Haobo Fu},
      year={2024},
      eprint={2402.02330},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.02330}, 
}


@Article{5,
AUTHOR = {Yu, Ping and Xu, Hua and Hu, Xia and Deng, Chao},
TITLE = {Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for Healthcare Integration},
JOURNAL = {Healthcare},
VOLUME = {11},
YEAR = {2023},
NUMBER = {20},
ARTICLE-NUMBER = {2776},
URL = {https://www.mdpi.com/2227-9032/11/20/2776},
PubMedID = {37893850},
ISSN = {2227-9032},
ABSTRACT = {Generative artificial intelligence (AI) and large language models (LLMs), exemplified by ChatGPT, are promising for revolutionizing data and information management in healthcare and medicine. However, there is scant literature guiding their integration for non-AI professionals. This study conducts a scoping literature review to address the critical need for guidance on integrating generative AI and LLMs into healthcare and medical practices. It elucidates the distinct mechanisms underpinning these technologies, such as Reinforcement Learning from Human Feedback (RLFH), including few-shot learning and chain-of-thought reasoning, which differentiates them from traditional, rule-based AI systems. It requires an inclusive, collaborative co-design process that engages all pertinent stakeholders, including clinicians and consumers, to achieve these benefits. Although global research is examining both opportunities and challenges, including ethical and legal dimensions, LLMs offer promising advancements in healthcare by enhancing data management, information retrieval, and decision-making processes. Continued innovation in data acquisition, model fine-tuning, prompt strategy development, evaluation, and system implementation is imperative for realizing the full potential of these technologies. Organizations should proactively engage with these technologies to improve healthcare quality, safety, and efficiency, adhering to ethical and legal guidelines for responsible application.},
DOI = {10.3390/healthcare11202776}
}

@misc{6,
      title={Game-theoretic LLM: Agent Workflow for Negotiation Games}, 
      author={Wenyue Hua and Ollie Liu and Lingyao Li and Alfonso Amayuelas and Julie Chen and Lucas Jiang and Mingyu Jin and Lizhou Fan and Fei Sun and William Wang and Xintong Wang and Yongfeng Zhang},
      year={2024},
      eprint={2411.05990},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2411.05990}, 
}


@misc{7,
  title={Towards LLM-guided Causal Explainability for Black-box Text Classifiers},
  author={Amrita Bhattacharjee and Raha Moraffah and Joshua Garland and Huan Liu},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:262459118}
}

@misc{8,
      title={MultiMath: Bridging Visual and Mathematical Reasoning for Large Language Models}, 
      author={Shuai Peng and Di Fu and Liangcai Gao and Xiuqin Zhong and Hongguang Fu and Zhi Tang},
      year={2024},
      eprint={2409.00147},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.00147}, 
}

@inproceedings{10,
    title = "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
    author = "Li, Huao  and
      Chong, Yu  and
      Stepputtis, Simon  and
      Campbell, Joseph  and
      Hughes, Dana  and
      Lewis, Charles  and
      Sycara, Katia",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.13/",
    doi = "10.18653/v1/2023.emnlp-main.13",
    pages = "180--192",
    abstract = "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents."
}

@misc{zeng2023largelanguagemodelsrobotics,
      title={Large Language Models for Robotics: A Survey}, 
      author={Fanlong Zeng and Wensheng Gan and Yongheng Wang and Ning Liu and Philip S. Yu},
      year={2023},
      eprint={2311.07226},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2311.07226}, 
}

@misc{wang2023voyageropenendedembodiedagent,
      title={Voyager: An Open-Ended Embodied Agent with Large Language Models}, 
      author={Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi Fan and Anima Anandkumar},
      year={2023},
      eprint={2305.16291},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2305.16291}, 
}

@misc{ahn2022icanisay,
      title={Do As I Can, Not As I Say: Grounding Language in Robotic Affordances}, 
      author={Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and Chuyuan Fu and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and Nikhil J Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and Sergey Levine and Yao Lu and Linda Luu and Carolina Parada and Peter Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and Diego Reyes and Pierre Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan and Andy Zeng},
      year={2022},
      eprint={2204.01691},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2204.01691}, 
}

@misc{liang2023codepolicieslanguagemodel,
      title={Code as Policies: Language Model Programs for Embodied Control}, 
      author={Jacky Liang and Wenlong Huang and Fei Xia and Peng Xu and Karol Hausman and Brian Ichter and Pete Florence and Andy Zeng},
      year={2023},
      eprint={2209.07753},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2209.07753}, 
}


@InProceedings{pmlr-v205-huang23c,
  title = 	 {Inner Monologue: Embodied Reasoning through Planning with Language Models},
  author =       {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and Sermanet, Pierre and Jackson, Tomas and Brown, Noah and Luu, Linda and Levine, Sergey and Hausman, Karol and ichter, brian},
  booktitle = 	 {Proceedings of The 6th Conference on Robot Learning},
  pages = 	 {1769--1782},
  year = 	 {2023},
  editor = 	 {Liu, Karen and Kulic, Dana and Ichnowski, Jeff},
  volume = 	 {205},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {14--18 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v205/huang23c/huang23c.pdf},
  url = 	 {https://proceedings.mlr.press/v205/huang23c.html},
  abstract = 	 {Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent’s own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.}
}

@misc{amodei2016concreteproblemsaisafety,
      title={Concrete Problems in AI Safety}, 
      author={Dario Amodei and Chris Olah and Jacob Steinhardt and Paul Christiano and John Schulman and Dan Mané},
      year={2016},
      eprint={1606.06565},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1606.06565}, 
}

@misc{raji2023concreteproblemsaisafety,
      title={Concrete Problems in AI Safety, Revisited}, 
      author={Inioluwa Deborah Raji and Roel Dobbe},
      year={2023},
      eprint={2401.10899},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2401.10899}, 
}

@misc{chevalierboisvert2019babyaiplatformstudysample,
      title={BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning}, 
      author={Maxime Chevalier-Boisvert and Dzmitry Bahdanau and Salem Lahlou and Lucas Willems and Chitwan Saharia and Thien Huu Nguyen and Yoshua Bengio},
      year={2019},
      eprint={1810.08272},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1810.08272}, 
}

@misc{shridhar2021alfworldaligningtextembodied,
      title={ALFWorld: Aligning Text and Embodied Environments for Interactive Learning}, 
      author={Mohit Shridhar and Xingdi Yuan and Marc-Alexandre Côté and Yonatan Bisk and Adam Trischler and Matthew Hausknecht},
      year={2021},
      eprint={2010.03768},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.03768}, 
}

@misc{mees2022calvinbenchmarklanguageconditionedpolicy,
      title={CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks}, 
      author={Oier Mees and Lukas Hermann and Erick Rosete-Beas and Wolfram Burgard},
      year={2022},
      eprint={2112.03227},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2112.03227}, 
}

@misc{küttler2020nethacklearningenvironment,
      title={The NetHack Learning Environment}, 
      author={Heinrich Küttler and Nantas Nardelli and Alexander H. Miller and Roberta Raileanu and Marco Selvatici and Edward Grefenstette and Tim Rocktäschel},
      year={2020},
      eprint={2006.13760},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.13760}, 
}

@article{JMLR:v16:garcia15a,
  author  = {Javier GarcÃ­a and Fernando FernÃ¡ndez},
  title   = {A Comprehensive Survey on Safe Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {42},
  pages   = {1437--1480},
  url     = {http://jmlr.org/papers/v16/garcia15a.html}
}